"use strict";(self.webpackChunkhertzbeat=self.webpackChunkhertzbeat||[]).push([[7239],{15680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>u});var i=t(96540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,i,o=function(e,n){if(null==e)return{};var t,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var c=i.createContext({}),s=function(e){var n=i.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=s(e.components);return i.createElement(c.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},m=i.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=s(t),u=o,g=m["".concat(c,".").concat(u)]||m[u]||d[u]||a;return t?i.createElement(g,r(r({ref:n},p),{},{components:t})):i.createElement(g,r({ref:n},p))}));function u(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,r=new Array(a);r[0]=m;var l={};for(var c in n)hasOwnProperty.call(n,c)&&(l[c]=n[c]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var s=2;s<a;s++)r[s]=t[s];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}m.displayName="MDXCreateElement"},57971:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>l,toc:()=>s});var i=t(58168),o=(t(96540),t(15680));const a={title:"How to Participate in Developing Custom Collectors",author:"zhangshenghang",author_title:"zhangshenghang",author_url:"https://github.com/zhangshenghang",tags:["opensource","practice"],keywords:["open source monitoring system","alerting system"]},r=void 0,l={permalink:"/blog/2024/11/24/custom-development",editUrl:"https://github.com/apache/hertzbeat/edit/master/home/blog/2024-11-24-custom-development.md",source:"@site/blog/2024-11-24-custom-development.md",title:"How to Participate in Developing Custom Collectors",description:"Introduction to the Collector Module",date:"2024-11-24T00:00:00.000Z",formattedDate:"November 24, 2024",tags:[{label:"opensource",permalink:"/blog/tags/opensource"},{label:"practice",permalink:"/blog/tags/practice"}],readingTime:11.49,hasTruncateMarker:!1,authors:[{name:"zhangshenghang",title:"zhangshenghang",url:"https://github.com/zhangshenghang"}],frontMatter:{title:"How to Participate in Developing Custom Collectors",author:"zhangshenghang",author_title:"zhangshenghang",author_url:"https://github.com/zhangshenghang",tags:["opensource","practice"],keywords:["open source monitoring system","alerting system"]},prevItem:{title:"GSOC Google Summer of Code 2025 Recruitment is Underway | We Look Forward to Your Proposals",permalink:"/blog/2025/03/03/gsoc-2025"},nextItem:{title:"Announcement of Apache HertzBeat\u2122 1.6.1 Release",permalink:"/blog/2024/11/09/hertzbeat-v1.6.1"}},c={authorsImageUrls:[void 0]},s=[{value:"Introduction to the Collector Module",id:"introduction-to-the-collector-module",level:2},{value:"Adding New Collector Monitoring",id:"adding-new-collector-monitoring",level:2},{value:"1. Creating the <code>kafka-collector</code> Module",id:"1-creating-the-kafka-collector-module",level:3},{value:"2. Adding the Kafka Protocol Class",id:"2-adding-the-kafka-protocol-class",level:3},{value:"3. Adding Kafka Support in Metrics",id:"3-adding-kafka-support-in-metrics",level:3},{value:"4. Adding Constants",id:"4-adding-constants",level:3},{value:"5. Adding the Kafka Connection Class",id:"5-adding-the-kafka-connection-class",level:3},{value:"6. Implementing the Kafka Collection Class",id:"6-implementing-the-kafka-collection-class",level:3},{value:"7. Configuring the SPI Service File",id:"7-configuring-the-spi-service-file",level:3},{value:"8. Adding Kafka Dependencies to the Collector Module",id:"8-adding-kafka-dependencies-to-the-collector-module",level:3},{value:"Adding Configuration Parsing Files",id:"adding-configuration-parsing-files",level:2},{value:"Development and Debugging",id:"development-and-debugging",level:2}],p={toc:s};function d(e){let{components:n,...a}=e;return(0,o.yg)("wrapper",(0,i.A)({},p,a,{components:n,mdxType:"MDXLayout"}),(0,o.yg)("h2",{id:"introduction-to-the-collector-module"},"Introduction to the Collector Module"),(0,o.yg)("p",null,(0,o.yg)("a",{target:"_blank",href:t(81041).A},"model-desc")),(0,o.yg)("p",null,"The overall structure of the Collector module can be divided into four main parts, each responsible for different tasks:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"Collector Entry Point"),": This is the entry point for running the Collector module, from which collection tasks are executed after startup.")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"collector-basic"),": This module mainly includes basic Collector implementations, such as monitoring for common protocols like HTTP and JDBC. The Collectors here typically do not require additional proprietary dependencies and can meet most basic monitoring needs.")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"collector-common"),": This module stores some general-purpose utility classes and methods, such as shared connection pools and caching mechanisms, which other modules can reuse.")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"collector-xxx"),": This is the extension Collector module for different services or protocols. For example, monitoring for specific services like MongoDB or RocketMQ often requires introducing their proprietary dependencies and developing within their respective modules. Below is an example of MongoDB's dependency:"),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>org.mongodb</groupId>\n    <artifactId>mongodb-driver-sync</artifactId>\n</dependency>\n")))),(0,o.yg)("p",null,"Through this modular design, the Collector can easily be extended to adapt to various monitoring scenarios."),(0,o.yg)("h2",{id:"adding-new-collector-monitoring"},"Adding New Collector Monitoring"),(0,o.yg)("p",null,"Next, we will demonstrate how to develop a new Collector through the practical case of creating a ",(0,o.yg)("inlineCode",{parentName:"p"},"kafka-collector")," module."),(0,o.yg)("h3",{id:"1-creating-the-kafka-collector-module"},"1. Creating the ",(0,o.yg)("inlineCode",{parentName:"h3"},"kafka-collector")," Module"),(0,o.yg)("p",null,"First, we need to create a new module in the project for Kafka monitoring, named ",(0,o.yg)("inlineCode",{parentName:"p"},"kafka-collector"),". Then, modify the ",(0,o.yg)("inlineCode",{parentName:"p"},"pom.xml")," file in this module.\n",(0,o.yg)("a",{target:"_blank",href:t(1898).A},"model-create")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},(0,o.yg)("inlineCode",{parentName:"strong"},"pom.xml")," Configuration")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n  <modelVersion>4.0.0</modelVersion>\n  <parent>\n    <groupId>org.apache.hertzbeat</groupId>\n    <artifactId>hertzbeat-collector</artifactId>\n    <version>2.0-SNAPSHOT</version>\n  </parent>\n\n  <artifactId>hertzbeat-collector-kafka</artifactId>\n  <name>${project.artifactId}</name>\n\n  <properties>\n    <maven.compiler.source>17</maven.compiler.source>\n    <maven.compiler.target>17</maven.compiler.target>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n  </properties>\n\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.hertzbeat</groupId>\n      <artifactId>hertzbeat-collector-common</artifactId>\n      <scope>provided</scope>\n    </dependency>\n    \x3c!-- kafka --\x3e\n    <dependency>\n      <groupId>org.apache.kafka</groupId>\n      <artifactId>kafka-clients</artifactId>\n    </dependency>\n  </dependencies>\n</project>\n')),(0,o.yg)("p",null,"Points to note:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Set ",(0,o.yg)("inlineCode",{parentName:"li"},"artifactId")," to ",(0,o.yg)("inlineCode",{parentName:"li"},"hertzbeat-collector-kafka")," to maintain naming consistency."),(0,o.yg)("li",{parentName:"ul"},"Manually add the dependencies required for Kafka in ",(0,o.yg)("inlineCode",{parentName:"li"},"dependencies"),".")),(0,o.yg)("h3",{id:"2-adding-the-kafka-protocol-class"},"2. Adding the Kafka Protocol Class"),(0,o.yg)("p",null,"To enable the Collector module to handle the Kafka monitoring protocol, we need to create a ",(0,o.yg)("inlineCode",{parentName:"p"},"KafkaProtocol")," class to define the connection parameters for Kafka. This class should be located at ",(0,o.yg)("inlineCode",{parentName:"p"},"common/src/main/java/org/apache/hertzbeat/common/entity/job/protocol/KafkaProtocol.java"),"."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},"package org.apache.hertzbeat.common.entity.job.protocol;\n\nimport lombok.AllArgsConstructor;\nimport lombok.Builder;\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\n\n@Data\n@Builder\n@AllArgsConstructor\n@NoArgsConstructor\npublic class KafkaProtocol {\n\n  /**\n   * IP address or domain name\n   */\n  private String host;\n\n  /**\n   * Port number\n   */\n  private String port;\n\n  /**\n   * Timeout\n   */\n  private String timeout;\n\n  /**\n   * Command\n   */\n  private String command;\n}\n")),(0,o.yg)("h3",{id:"3-adding-kafka-support-in-metrics"},"3. Adding Kafka Support in Metrics"),(0,o.yg)("p",null,"In the ",(0,o.yg)("inlineCode",{parentName:"p"},"common/src/main/java/org/apache/hertzbeat/common/entity/job/Metrics.java")," class, add support for the Kafka protocol."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},"private KafkaProtocol kclient;\n")),(0,o.yg)("h3",{id:"4-adding-constants"},"4. Adding Constants"),(0,o.yg)("p",null,"Define constants for the Kafka protocol in the ",(0,o.yg)("inlineCode",{parentName:"p"},"DispatchConstants")," class."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},'String PROTOCOL_KAFKA = "kclient";\n')),(0,o.yg)("h3",{id:"5-adding-the-kafka-connection-class"},"5. Adding the Kafka Connection Class"),(0,o.yg)("p",null,"The ",(0,o.yg)("inlineCode",{parentName:"p"},"KafkaConnect")," class is used to manage the connection logic for the Kafka Admin."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},"package org.apache.hertzbeat.collector.collect.kafka;\n\nimport org.apache.hertzbeat.collector.collect.common.cache.AbstractConnection;\nimport org.apache.kafka.clients.admin.AdminClient;\nimport org.apache.kafka.clients.admin.AdminClientConfig;\nimport org.apache.kafka.clients.admin.KafkaAdminClient;\n\nimport java.util.Properties;\n\npublic class KafkaConnect extends AbstractConnection<AdminClient> {\n\n  private static AdminClient adminClient;\n\n  public KafkaConnect(String brokerList) {\n    Properties properties = new Properties();\n    properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);\n    properties.put(AdminClientConfig.RETRIES_CONFIG, 3);\n    adminClient = KafkaAdminClient.create(properties);\n  }\n\n  @Override\n  public AdminClient getConnection() {\n    return adminClient;\n  }\n\n  @Override\n  public void closeConnection() throws Exception {\n    if (this.adminClient != null) {\n      this.adminClient.close();\n    }\n  }\n\n  public static synchronized AdminClient getAdminClient(String brokerList) {\n    if (adminClient == null) {\n      Properties properties = new Properties();\n      properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);\n      adminClient = KafkaAdminClient.create(properties);\n    }\n    return adminClient;\n  }\n\n}\n")),(0,o.yg)("h3",{id:"6-implementing-the-kafka-collection-class"},"6. Implementing the Kafka Collection Class"),(0,o.yg)("p",null,"Inherit from the ",(0,o.yg)("inlineCode",{parentName:"p"},"AbstractCollect")," class and implement the specific data collection logic within it. Specific logic details are not covered here."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},'/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the "License"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an "AS IS" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.hertzbeat.collector.collect.kafka;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.hertzbeat.collector.collect.AbstractCollect;\nimport org.apache.hertzbeat.collector.dispatch.DispatchConstants;\nimport org.apache.hertzbeat.common.entity.job.Metrics;\nimport org.apache.hertzbeat.common.entity.job.protocol.KafkaProtocol;\nimport org.apache.hertzbeat.common.entity.message.CollectRep;\nimport org.apache.kafka.clients.admin.AdminClient;\nimport org.apache.kafka.clients.admin.DescribeTopicsResult;\nimport org.apache.kafka.clients.admin.ListTopicsOptions;\nimport org.apache.kafka.clients.admin.ListTopicsResult;\nimport org.apache.kafka.clients.admin.OffsetSpec;\nimport org.apache.kafka.clients.admin.TopicDescription;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.common.TopicPartitionInfo;\nimport org.springframework.util.Assert;\n\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\n\n@Slf4j\npublic class KafkaCollectImpl extends AbstractCollect {\n\n  @Override\n  public void preCheck(Metrics metrics) throws IllegalArgumentException {\n    KafkaProtocol kafkaProtocol = metrics.getKclient();\n    // Ensure that metrics and kafkaProtocol are not null\n    Assert.isTrue(metrics != null && kafkaProtocol != null, "Kafka collect must have kafkaProtocol params");\n    // Ensure that host and port are not empty\n    Assert.hasText(kafkaProtocol.getHost(), "Kafka Protocol host is required.");\n    Assert.hasText(kafkaProtocol.getPort(), "Kafka Protocol port is required.");\n  }\n\n  @Override\n  public void collect(CollectRep.MetricsData.Builder builder, long monitorId, String app, Metrics metrics) {\n    try {\n      KafkaProtocol kafkaProtocol = metrics.getKclient();\n      String command = kafkaProtocol.getCommand();\n      boolean isKafkaCommand = SupportedCommand.isKafkaCommand(command);\n      if (!isKafkaCommand) {\n        log.error("Unsupported command: {}", command);\n        return;\n      }\n\n      // Create AdminClient with the provided host and port\n      AdminClient adminClient = KafkaConnect.getAdminClient(kafkaProtocol.getHost() + ":" + kafkaProtocol.getPort());\n\n      // Execute the appropriate collection method based on the command\n      switch (SupportedCommand.fromCommand(command)) {\n        case TOPIC_DESCRIBE:\n          collectTopicDescribe(builder, adminClient);\n          break;\n        case TOPIC_LIST:\n          collectTopicList(builder, adminClient);\n          break;\n        case TOPIC_OFFSET:\n          collectTopicOffset(builder, adminClient);\n          break;\n        default:\n          log.error("Unsupported command: {}", command);\n          break;\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      log.error("Kafka collect error", e);\n    }\n  }\n\n  /**\n   * Collect the earliest and latest offsets for each topic\n   *\n   * @param builder     The MetricsData builder\n   * @param adminClient The AdminClient\n   * @throws InterruptedException If the thread is interrupted\n   * @throws ExecutionException   If an error occurs during execution\n   */\n  private void collectTopicOffset(CollectRep.MetricsData.Builder builder, AdminClient adminClient) throws InterruptedException, ExecutionException {\n    ListTopicsResult listTopicsResult = adminClient.listTopics(new ListTopicsOptions().listInternal(true));\n    Set<String> names = listTopicsResult.names().get();\n    names.forEach(name -> {\n      try {\n        Map<String, TopicDescription> map = adminClient.describeTopics(Collections.singleton(name)).all().get(3L, TimeUnit.SECONDS);\n        map.forEach((key, value) -> value.partitions().forEach(info -> extractedOffset(builder, adminClient, name, value, info)));\n      } catch (TimeoutException | InterruptedException | ExecutionException e) {\n        log.warn("Topic {} get offset fail", name);\n      }\n    });\n  }\n\n  private void extractedOffset(CollectRep.MetricsData.Builder builder, AdminClient adminClient, String name, TopicDescription value, TopicPartitionInfo info) {\n    try {\n      TopicPartition topicPartition = new TopicPartition(value.name(), info.partition());\n      long earliestOffset = getEarliestOffset(adminClient, topicPartition);\n      long latestOffset = getLatestOffset(adminClient, topicPartition);\n      CollectRep.ValueRow.Builder valueRowBuilder = CollectRep.ValueRow.newBuilder();\n      valueRowBuilder.addColumns(value.name());\n      valueRowBuilder.addColumns(String.valueOf(info.partition()));\n      valueRowBuilder.addColumns(String.valueOf(earliestOffset));\n      valueRowBuilder.addColumns(String.valueOf(latestOffset));\n      builder.addValues(valueRowBuilder.build());\n    } catch (TimeoutException | InterruptedException | ExecutionException e) {\n      log.warn("Topic {} get offset fail", name);\n    }\n  }\n\n  /**\n   * Get the earliest offset for a given topic partition\n   *\n   * @param adminClient    The AdminClient\n   * @param topicPartition The TopicPartition\n   * @return The earliest offset\n   */\n  private long getEarliestOffset(AdminClient adminClient, TopicPartition topicPartition)\n          throws InterruptedException, ExecutionException, TimeoutException {\n    return adminClient\n            .listOffsets(Collections.singletonMap(topicPartition, OffsetSpec.earliest()))\n            .all()\n            .get(3L, TimeUnit.SECONDS)\n            .get(topicPartition)\n            .offset();\n  }\n\n  /**\n   * Get the latest offset for a given topic partition\n   *\n   * @param adminClient    The AdminClient\n   * @param topicPartition The TopicPartition\n   * @return The latest offset\n   */\n  private long getLatestOffset(AdminClient adminClient, TopicPartition topicPartition)\n          throws InterruptedException, ExecutionException, TimeoutException {\n    return adminClient\n            .listOffsets(Collections.singletonMap(topicPartition, OffsetSpec.latest()))\n            .all()\n            .get(3L, TimeUnit.SECONDS)\n            .get(topicPartition)\n            .offset();\n  }\n\n  /**\n   * Collect the list of topics\n   *\n   * @param builder     The MetricsData builder\n   * @param adminClient The AdminClient\n   */\n  private static void collectTopicList(CollectRep.MetricsData.Builder builder, AdminClient adminClient) throws InterruptedException, ExecutionException {\n    ListTopicsOptions options = new ListTopicsOptions().listInternal(true);\n    Set<String> names = adminClient.listTopics(options).names().get();\n    names.forEach(name -> {\n      CollectRep.ValueRow valueRow = CollectRep.ValueRow.newBuilder().addColumns(name).build();\n      builder.addValues(valueRow);\n    });\n  }\n\n  /**\n   * Collect the description of each topic\n   *\n   * @param builder     The MetricsData builder\n   * @param adminClient The AdminClient\n   */\n  private static void collectTopicDescribe(CollectRep.MetricsData.Builder builder, AdminClient adminClient) throws InterruptedException, ExecutionException {\n    ListTopicsOptions options = new ListTopicsOptions();\n    options.listInternal(true);\n    ListTopicsResult listTopicsResult = adminClient.listTopics(options);\n    Set<String> names = listTopicsResult.names().get();\n    DescribeTopicsResult describeTopicsResult = adminClient.describeTopics(names);\n    Map<String, TopicDescription> map = describeTopicsResult.all().get();\n    map.forEach((key, value) -> {\n      List<TopicPartitionInfo> listp = value.partitions();\n      listp.forEach(info -> {\n        CollectRep.ValueRow.Builder valueRowBuilder = CollectRep.ValueRow.newBuilder();\n        valueRowBuilder.addColumns(value.name());\n        valueRowBuilder.addColumns(String.valueOf(value.partitions().size()));\n        valueRowBuilder.addColumns(String.valueOf(info.partition()));\n        valueRowBuilder.addColumns(info.leader().host());\n        valueRowBuilder.addColumns(String.valueOf(info.leader().port()));\n        valueRowBuilder.addColumns(String.valueOf(info.replicas().size()));\n        valueRowBuilder.addColumns(String.valueOf(info.replicas()));\n        builder.addValues(valueRowBuilder.build());\n      });\n    });\n  }\n\n  @Override\n  public String supportProtocol() {\n    return DispatchConstants.PROTOCOL_KAFKA;\n  }\n}\n')),(0,o.yg)("h3",{id:"7-configuring-the-spi-service-file"},"7. Configuring the SPI Service File"),(0,o.yg)("p",null,"In the ",(0,o.yg)("inlineCode",{parentName:"p"},"collector/collector/src/main/resources/META-INF/services/org.apache.hertzbeat.collector.collect.AbstractCollect")," file, add the ",(0,o.yg)("inlineCode",{parentName:"p"},"KafkaCollectImpl")," class."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-text"},"...\norg.apache.hertzbeat.collector.collect.kafka.KafkaCollectImpl\n")),(0,o.yg)("h3",{id:"8-adding-kafka-dependencies-to-the-collector-module"},"8. Adding Kafka Dependencies to the Collector Module"),(0,o.yg)("p",null,"The final step is to add the ",(0,o.yg)("inlineCode",{parentName:"p"},"kafka-collector")," module dependency in ",(0,o.yg)("inlineCode",{parentName:"p"},"collector/collector/pom.xml"),":"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>org.apache.hertzbeat</groupId>\n  <artifactId>hertzbeat-collector-kafka</artifactId>\n  <version>${hertzbeat.version}</version>\n</dependency>\n")),(0,o.yg)("p",null,"By following the above steps, we have completed the development of a Kafka Collector, from protocol definition to the final SPI configuration and dependency management, fully extending a Kafka monitoring module."),(0,o.yg)("h2",{id:"adding-configuration-parsing-files"},"Adding Configuration Parsing Files"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-yaml"},"# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# The monitoring type category: service-application service monitoring db-database monitoring custom-custom monitoring os-operating system monitoring\ncategory: mid\n# The monitoring type e.g.: linux windows tomcat mysql aws...\napp: kafka_client\n# The monitoring i18n name\nname:\n  zh-CN: Kafka\u6d88\u606f\u7cfb\u7edf\uff08\u5ba2\u6237\u7aef\uff09\n  en-US: Kafka Message (Client)\n  zh-TW: Kafka\u6d88\u606f\u7cfb\u7d71\uff08\u5ba2\u6236\u7aef\uff09\n# The description and help of this monitoring type\nhelp:\n  zh-CN: HertzBeat \u4f7f\u7528 <a href=\"https://hertzbeat.apache.org/docs/advanced/extend-jmx\">Kafka Admin Client</a> \u5bf9 Kafka \u7684\u901a\u7528\u6307\u6807\u8fdb\u884c\u91c7\u96c6\u76d1\u63a7\u3002</span>\n  en-US: HertzBeat uses <a href='https://hertzbeat.apache.org/docs/advanced/extend-jmx'>Kafka Admin Client</a> to monitor Kafka general metrics. </span>\n  zh-TW: HertzBeat \u4f7f\u7528 <a href=\"https://hertzbeat.apache.org/docs/advanced/extend-jmx\">Kafka Admin Client</a> \u5c0d Kafka \u7684\u901a\u7528\u6307\u6a19\u9032\u884c\u91c7\u96c6\u76e3\u63a7\u3002</span>\nhelpLink:\n  zh-CN: https://hertzbeat.apache.org/zh-cn/docs/help/kafka_client\n  en-US: https://hertzbeat.apache.org/docs/help/kafka_client\n# Input params define for monitoring (render web UI by the definition)\nparams:\n  # field-param field key\n  - field: host\n    # name-param field display i18n name\n    name:\n      zh-CN: \u76ee\u6807Host\n      en-US: Target Host\n    # type-param field type (most mapping the HTML input type)\n    type: host\n    # required-true or false\n    required: true\n  - field: port\n    name:\n      zh-CN: \u7aef\u53e3\n      en-US: Port\n    type: number\n    # when type is number, range is required\n    range: '[0,65535]'\n    required: true\n    defaultValue: 9092\n\n# collect metrics config list\nmetrics:\n  # metrics - server_info\n  - name: topic_list\n    i18n:\n      zh-CN: \u4e3b\u9898\u5217\u8868\n      en-US: Topic List\n    # metrics scheduling priority (0->127) -> (high->low), metrics with the same priority will be scheduled in parallel\n    # priority 0's metrics are availability metrics, they will be scheduled first, only if availability metrics collect successfully will the scheduling continue\n    priority: 0\n    # collect metrics content\n    fields:\n      # field-metric name, type-metric type (0-number, 1-string), unit-metric unit ('%', 'ms', 'MB'), label-whether it is a metrics label field\n      - field: TopicName\n        type: 1\n        i18n:\n          zh-CN: \u4e3b\u9898\u540d\u79f0\n          en-US: Topic Name\n    # the protocol used for monitoring, e.g., sql, ssh, http, telnet, wmi, snmp, sdk\n    protocol: kclient\n    # the config content when protocol is jmx\n    kclient:\n      host: ^_^host^_^\n      port: ^_^port^_^\n      command: topic-list\n  - name: topic_detail\n    i18n:\n      zh-CN: \u4e3b\u9898\u8be6\u7ec6\u4fe1\u606f\n      en-US: Topic Detail Info\n    # metrics scheduling priority (0->127) -> (high->low), metrics with the same priority will be scheduled in parallel\n    # priority 0's metrics are availability metrics, they will be scheduled first, only if availability metrics collect successfully will the scheduling continue\n    priority: 0\n    # collect metrics content\n    fields:\n      # field-metric name, type-metric type (0-number, 1-string), unit-metric unit ('%', 'ms', 'MB'), label-whether it is a metrics label field\n      - field: TopicName\n        type: 1\n        i18n:\n          zh-CN: \u4e3b\u9898\u540d\u79f0\n          en-US: Topic Name\n      - field: PartitionNum\n        type: 1\n        i18n:\n          zh-CN: \u5206\u533a\u6570\u91cf\n          en-US: Partition Num\n      - field: PartitionLeader\n        type: 1\n        i18n:\n          zh-CN: \u5206\u533a\u9886\u5bfc\u8005\n          en-US: Partition Leader\n      - field: BrokerHost\n        type: 1\n        i18n:\n          zh-CN: Broker\u4e3b\u673a\n          en-US: Broker Host\n      - field: BrokerPort\n        type: 1\n        i18n:\n          zh-CN: Broker\u7aef\u53e3\n          en-US: Broker Port\n      - field: ReplicationFactorSize\n        type: 1\n        i18n:\n          zh-CN: \u590d\u5236\u56e0\u5b50\u5927\u5c0f\n          en-US: Replication Factor Size\n      - field: ReplicationFactor\n        type: 1\n        i18n:\n          zh-CN: \u590d\u5236\u56e0\u5b50\n          en-US: Replication Factor\n    # the protocol used for monitoring, e.g., sql, ssh, http, telnet, wmi, snmp, sdk\n    protocol: kclient\n    # the config content when protocol is jmx\n    kclient:\n      host: ^_^host^_^\n      port: ^_^port^_^\n      command: topic-describe\n  - name: topic_offset\n    i18n:\n      zh-CN: \u4e3b\u9898\u504f\u79fb\u91cf\n      en-US: Topic Offset\n    # metrics scheduling priority (0->127) -> (high->low), metrics with the same priority will be scheduled in parallel\n    # priority 0's metrics are availability metrics, they will be scheduled first, only if availability metrics collect successfully will the scheduling continue\n    priority: 0\n    # collect metrics content\n    fields:\n      # field-metric name, type-metric type (0-number, 1-string), unit-metric unit ('%', 'ms', 'MB'), label-whether it is a metrics label field\n      - field: TopicName\n        type: 1\n        i18n:\n          zh-CN: \u4e3b\u9898\u540d\u79f0\n          en-US: Topic Name\n      - field: PartitionNum\n        type: 1\n        i18n:\n          zh-CN: \u5206\u533a\u6570\u91cf\n          en-US: Partition Num\n      - field: earliest\n        type: 0\n        i18n:\n          zh-CN: \u6700\u65e9\u504f\u79fb\u91cf\n          en-US: Earliest Offset\n      - field: latest\n        type: 0\n        i18n:\n          zh-CN: \u6700\u65b0\u504f\u79fb\u91cf\n          en-US: Latest Offset\n    # the protocol used for monitoring, e.g., sql, ssh, http, telnet, wmi, snmp, sdk\n    protocol: kclient\n    # the config content when protocol is jmx\n    kclient:\n      host: ^_^host^_^\n      port: ^_^port^_^\n      command: topic-offset\n")),(0,o.yg)("p",null,"With these steps, the custom development of the ",(0,o.yg)("inlineCode",{parentName:"p"},"collector")," is complete. You can start the service and begin monitoring metrics according to the normal logic."),(0,o.yg)("h2",{id:"development-and-debugging"},"Development and Debugging"),(0,o.yg)("p",null,"When starting the ",(0,o.yg)("inlineCode",{parentName:"p"},"manager")," module locally, if the added monitoring cannot find the class, add the dependency to the ",(0,o.yg)("inlineCode",{parentName:"p"},"manager")," module."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Note: When packaging and submitting code, do not submit the dependencies under the ",(0,o.yg)("inlineCode",{parentName:"strong"},"manager")," module.")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-xml"},"\x3c!-- collector-kafka --\x3e\n<dependency>\n  <groupId>org.apache.hertzbeat</groupId>\n  <artifactId>hertzbeat-collector-kafka</artifactId>\n  <version>${hertzbeat.version}</version>\n</dependency>\n")))}d.isMDXComponent=!0},1898:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/files/model-create-26dd8d16e6bfc29a7cb414169a832feb.png"},81041:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/files/model-desc-d1a4d60dbef4934877da653ae2a1eb75.png"}}]);